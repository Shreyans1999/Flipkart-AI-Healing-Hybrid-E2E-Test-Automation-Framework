# üöÄ AI-Powered Self-Healing Playwright Automation Framework

> **Enterprise-Grade End-to-End Test Automation using Playwright, TypeScript & AI (LLMs)**

[![Playwright Tests](https://github.com/your-repo/ai-self-healing-playwright/actions/workflows/playwright-ci.yml/badge.svg)](https://github.com/your-repo/ai-self-healing-playwright/actions)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.3-blue)](https://www.typescriptlang.org/)
[![Playwright](https://img.shields.io/badge/Playwright-1.40-green)](https://playwright.dev/)

---

## üìå Overview

Modern web applications evolve rapidly. UI changes such as updated IDs, modified DOM structures, or redesigned components often cause **automation test failures**, leading to flaky tests, broken CI pipelines, and high maintenance overhead.

This project introduces an **AI-powered Self-Healing Test Automation Framework** built using **Playwright + TypeScript**, designed to **automatically detect, heal, and recover from locator failures at runtime** using **LLM-based reasoning**.

---

## üéØ Key Features

- ‚úÖ **Self-Healing Locators** - Automatically recover from broken selectors
- ‚úÖ **AI-Powered** - Uses OpenAI/Azure/Ollama for intelligent selector generation
- ‚úÖ **Fallback Strategy** - Multiple fallback selectors per element
- ‚úÖ **Auto-Update** - Automatically updates locator files after healing
- ‚úÖ **Structured Logging** - Winston-based JSON logging with healing analytics
- ‚úÖ **Allure Reporting** - Rich test reports with healing annotations
- ‚úÖ **CI/CD Ready** - GitHub Actions pipeline with parallel execution
- ‚úÖ **Page Object Model** - Clean, maintainable test architecture

---

## üèóÔ∏è Architecture

```
Test Layer (E2E Specs)
       ‚Üì
Page Object Layer (UI Actions)
       ‚Üì
Smart Locator Resolver
       ‚Üì
AI Self-Healing Engine
       ‚Üì
Utilities / Config / Logging / Reporting
```

---

## üìÅ Project Structure

```
ai-self-healing-playwright/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ tests/e2e/           # E2E test specifications
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ login.spec.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ checkout.spec.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ user-flow.spec.ts
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ pages/               # Page Object classes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ BasePage.ts      # Base with smart actions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LoginPage.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ DashboardPage.ts
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ locators/            # JSON locator files
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ login.locators.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ dashboard.locators.json
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ai/                  # Self-Healing Engine
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ HealingEngine.ts # Main orchestrator
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LLMClient.ts     # OpenAI/Azure/Ollama
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LocatorAnalyzer.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ HealingStrategy.ts
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ core/                # Framework core
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ TestSetup.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ RetryHandler.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ PlaywrightHooks.ts
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ utils/               # Utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Logger.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ DomSnapshot.ts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ FileUtils.ts
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ EnvLoader.ts
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ config/              # Configuration
‚îÇ       ‚îú‚îÄ‚îÄ playwright.config.ts
‚îÇ       ‚îú‚îÄ‚îÄ env.dev.ts
‚îÇ       ‚îú‚îÄ‚îÄ env.qa.ts
‚îÇ       ‚îî‚îÄ‚îÄ env.prod.ts
‚îÇ
‚îú‚îÄ‚îÄ reports/                 # Test reports
‚îú‚îÄ‚îÄ logs/                    # Log files
‚îú‚îÄ‚îÄ .github/workflows/       # CI/CD
‚îú‚îÄ‚îÄ playwright.config.ts     # Root config
‚îú‚îÄ‚îÄ package.json
‚îú‚îÄ‚îÄ tsconfig.json
‚îî‚îÄ‚îÄ .env
```

---

## üöÄ Quick Start

### Prerequisites

- Node.js 18+
- npm or yarn
- OpenAI API key (or Azure OpenAI/Ollama for local)

### Installation

```bash
# Clone the repository
git clone https://github.com/your-repo/ai-self-healing-playwright.git
cd ai-self-healing-playwright

# Install dependencies
npm install

# Install Playwright browsers
npx playwright install
```

### Configuration

1. Copy `.env` and add your API key:
```bash
# .env
OPENAI_API_KEY=your-api-key-here
LLM_PROVIDER=openai
```

2. Run tests:
```bash
# Run all tests
npm test

# Run with UI
npm run test:ui

# Run headed
npm run test:headed

# Run specific browser
npm run test:chromium
```

---

## üîß Self-Healing Flow

When a locator fails, the framework:

1. **Detects Failure** - Catches Playwright exceptions
2. **Tries Fallbacks** - Attempts fallback selectors from JSON
3. **Captures DOM** - Snapshots page structure
4. **Queries LLM** - Asks AI for alternative selectors
5. **Validates** - Tests proposed selectors in browser
6. **Updates File** - Saves working selector to JSON
7. **Retries** - Re-executes the failed action
8. **Logs Event** - Records healing for analytics

```
Test Step
 ‚Üí Locator Lookup
 ‚Üí Action Attempt
 ‚Üí Failure Detected ‚ùå
 ‚Üí AI Healing Engine üîß
 ‚Üí Locator Validation ‚úì
 ‚Üí Retry Action
 ‚Üí Continue Test ‚úÖ
```

---

## üìù Locator Format

Locators are stored in JSON with primary and fallback selectors:

```json
{
  "loginButton": {
    "primary": "#login-btn",
    "fallbacks": [
      "button:has-text('Login')",
      "[data-testid='login-button']",
      "button[type='submit']"
    ]
  }
}
```

---

## üß™ Writing Tests

Tests use Page Objects with clean APIs:

```typescript
test('User login flow', async ({ page }, testInfo) => {
  const loginPage = new LoginPage(page, testInfo);
  const dashboardPage = new DashboardPage(page, testInfo);

  // Clean, readable test - no locators!
  await loginPage.navigateToLogin();
  await loginPage.login('tomsmith', 'SuperSecretPassword!');
  
  await dashboardPage.waitForDashboard();
  expect(await dashboardPage.getWelcomeMessage()).toContain('secure area');
});
```

---

## ‚öôÔ∏è Environment Configuration

| Variable | Description | Default |
|----------|-------------|---------|
| `LLM_PROVIDER` | AI provider (openai/azure/ollama) | `openai` |
| `OPENAI_API_KEY` | OpenAI API key | - |
| `TEST_ENV` | Environment (dev/qa/prod) | `dev` |
| `BASE_URL` | Application base URL | `https://the-internet.herokuapp.com` |
| `MAX_HEALING_RETRIES` | Max healing attempts | `3` |
| `HEALING_CONFIDENCE_THRESHOLD` | Min confidence for healing | `0.7` |
| `AUTO_UPDATE_LOCATORS` | Auto-update locator files | `true` |

---

## üìä Reporting

### Allure Report
```bash
npm run allure:generate
npm run allure:open
```

### Healing Logs
Check `logs/healing.log` for detailed healing events:
```json
{
  "level": "info",
  "message": "Healing succeeded for loginButton",
  "originalSelector": "#broken-selector",
  "healedSelector": "button:has-text('Login')",
  "confidenceScore": 0.85,
  "duration": 1234
}
```

---

## üö¶ CI/CD

GitHub Actions workflow runs:
- On push to main/master
- On pull requests
- Tests all browsers in parallel
- Uploads Allure reports
- Captures healing analytics

---

## üîå LLM Providers

### OpenAI (Default)
```env
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-xxx
OPENAI_MODEL=gpt-4-turbo-preview
```

### Ollama (Local - Free) ‚≠ê Recommended

Ollama allows you to run AI models locally for free. Here's the complete setup guide:

#### Step 1: Install Ollama

**macOS (Homebrew):**
```bash
brew install ollama
```

**macOS/Linux (Official):**
```bash
curl -fsSL https://ollama.ai/install.sh | sh
```

**Windows:**
Download from [ollama.ai/download](https://ollama.ai/download)

#### Step 2: Start Ollama Server

```bash
# Start the server (keep this terminal open)
ollama serve
```

The server runs at `http://localhost:11434` by default.

#### Step 3: Pull the Model

```bash
# Recommended model for selector healing (~4.9GB download)
ollama pull llama3.1:8b
```

**Alternative Models:**
| Model | Size | Speed | Quality |
|-------|------|-------|---------|
| `llama3.1:8b` | 4.9GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| `mistral:7b` | 4GB | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| `codellama:13b` | 7GB | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |
| `qwen2.5:7b` | 4.4GB | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |

#### Step 4: Configure .env

```env
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
```

#### Step 5: Verify Setup

```bash
# Test the model
ollama run llama3.1:8b "Hello, can you generate a CSS selector?"

# Check running models
ollama list
```

#### Troubleshooting

| Issue | Solution |
|-------|----------|
| "ollama server not responding" | Run `ollama serve` in a separate terminal |
| "model not found" | Run `ollama pull llama3.1:8b` |
| Slow responses | Try `mistral:7b` for faster inference |
| Out of memory | Use a smaller model like `llama3.2:3b` |

---

## üèÜ Best Practices

- ‚úÖ Clean architecture with SOLID principles
- ‚úÖ Zero hard-coded selectors
- ‚úÖ Environment isolation
- ‚úÖ AI used only when failures occur
- ‚úÖ Interview-ready, maintainable codebase

---

## üì¨ Future Enhancements

- [ ] Visual AI comparison
- [ ] Historical flaky test analytics
- [ ] Confidence-based healing approval
- [ ] Dashboard for healing metrics
- [ ] Slack/Teams notifications

---

## üë®‚Äçüíª Author

**SDET / QA Automation Engineer**  
Focused on scalable test architecture, CI/CD stability, and AI-driven automation solutions.

---

## üìÑ License

MIT License
